import sys
import io
import os
from contextlib import asynccontextmanager

# --- C·∫§U H√åNH UTF-8 ---
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    os.environ["PYTHONIOENCODING"] = "utf-8"

import logging
import urllib.parse
import asyncio
from mcp.server.fastmcp import FastMCP
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("MCP_Traffic_Server")

playwright_instance = None
browser_instance = None

@asynccontextmanager
async def server_lifespan(server: FastMCP):
    global playwright_instance, browser_instance
    logger.info("üöÄ Kh·ªüi ƒë·ªông Playwright (V6 - Nuclear Mode)...")
    try:
        playwright_instance = await async_playwright().start()
        # Headless=True. N·∫øu v·∫´n l·ªói, h√£y th·ª≠ ƒë·ªïi th√†nh False ƒë·ªÉ xem n√≥ l√†m g√¨
        browser_instance = await playwright_instance.chromium.launch(headless=False)
        logger.info("‚úÖ Browser Ready!")
        yield 
    finally:
        if browser_instance: await browser_instance.close()
        if playwright_instance: await playwright_instance.stop()

app = FastMCP("traffic_law_mcp", lifespan=server_lifespan)

@app.tool()
async def luat_lookup(query: str) -> str:
    """
    Tra c·ª©u lu·∫≠t: T√¨m ki·∫øm r·ªông v√† l·∫•y d·ªØ li·ªáu th√¥ n·∫øu c·∫ßn.
    """
    global browser_instance
    if not browser_instance: return "‚ùå L·ªói: Browser ch∆∞a ch·∫°y."

    # 1. B·ªé site:thuvienphapluat.vn ƒë·ªÉ t√¨m r·ªông h∆°n (LuatVietnam, BaoChinhPhu, v.v.)
    # ƒêi·ªÅu n√†y gi√∫p tƒÉng kh·∫£ nƒÉng t√¨m th·∫•y snippet c√≥ ch·ª©a con s·ªë
    search_query = f"{query} m·ª©c ph·∫°t ngh·ªã ƒë·ªãnh 100 123 m·ªõi nh·∫•t 2025"
    logger.info(f"üîç Search: {search_query}")
    
    page = None
    try:
        page = await browser_instance.new_page()
        # User-Agent c·ªßa m√°y t√≠nh th·∫≠t
        await page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        })

        q = urllib.parse.quote(search_query)
        # D√πng DuckDuckGo b·∫£n HTML (nh·∫π h∆°n, d·ªÖ l·∫•y text h∆°n b·∫£n JS)
        await page.goto(f"https://html.duckduckgo.com/html/?q={q}", timeout=20000)
        
        # --- CHI·∫æN THU·∫¨T V√âT C·∫†N D·ªÆ LI·ªÜU ---
        content = await page.content()
        soup = BeautifulSoup(content, "html.parser")
        
        # 1. Th·ª≠ l·∫•y c√°c k·∫øt qu·∫£ chu·∫©n (class .result__body ho·∫∑c .result__snippet)
        snippets = []
        valid_links = []
        
        # Qu√©t t·∫•t c·∫£ th·∫ª a (link) v√† div text
        for result in soup.select(".result"):
            title = result.select_one(".result__title")
            snippet = result.select_one(".result__snippet")
            url = result.select_one(".result__url")
            
            text_content = ""
            if title: text_content += title.get_text(" ", strip=True) + ". "
            if snippet: text_content += snippet.get_text(" ", strip=True)
            
            if text_content:
                snippets.append(f"- {text_content}")
            
            if url:
                href = url.get_text().strip()
                if "thuvienphapluat.vn" in href: valid_links.append(href)

        # 2. D·ªØ li·ªáu d·ª± ph√≤ng (Backup): N·∫øu kh√¥ng b·∫Øt ƒë∆∞·ª£c class, l·∫•y TO√ÄN B·ªò TEXT
        backup_data = "\n".join(snippets[:5])
        if len(backup_data) < 50:
             # L·∫•y to√†n b·ªô ch·ªØ tr√™n trang, x√≥a kho·∫£ng tr·∫Øng th·ª´a
             raw_text = soup.get_text(" ", strip=True)
             # C·∫Øt ƒëo·∫°n gi·ªØa (th∆∞·ªùng l√† k·∫øt qu·∫£ t√¨m ki·∫øm)
             mid = len(raw_text) // 2
             start = max(0, mid - 1000)
             backup_data = "D·ªÆ LI·ªÜU TH√î T·ª™ T√åM KI·∫æM:\n" + raw_text[start : start + 2000]

        # 3. ∆Øu ti√™n v√†o Link TVPL n·∫øu c√≥ (nh∆∞ng kh√¥ng b·∫Øt bu·ªôc)
        detail_content = ""
        if valid_links:
            target_link = "https://" + valid_links[0] if not valid_links[0].startswith("http") else valid_links[0]
            logger.info(f"üìñ Th·ª≠ v√†o: {target_link}")
            try:
                await page.goto(target_link, timeout=15000)
                sub_soup = BeautifulSoup(await page.content(), "html.parser")
                # L·∫•y text c·ªßa body, b·ªè script
                for s in sub_soup(["script", "style"]): s.decompose()
                
                # T√¨m v√πng n·ªôi dung ch√≠nh (th·ª≠ nhi·ªÅu class)
                main_div = (sub_soup.select_one(".content-0") or 
                            sub_soup.select_one(".news-content") or 
                            sub_soup.select_one("article") or 
                            sub_soup.body)
                
                detail_content = " ".join(main_div.get_text(" ", strip=True).split())[:3000]
            except:
                pass # L·ªói th√¨ b·ªè qua, d√πng backup

        # T·ªîNG H·ª¢P
        if detail_content and len(detail_content) > 200:
            return f"Ngu·ªìn: VƒÉn b·∫£n ph√°p lu·∫≠t\nLink: {valid_links[0]}\n\nCHI TI·∫æT:\n{detail_content}"
        else:
            return f"‚ö†Ô∏è T√¨m th·∫•y th√¥ng tin t√≥m t·∫Øt (AI h√£y t·ª± t·ªïng h·ª£p m·ª©c ph·∫°t t·ª´ ƒë√¢y):\n\n{backup_data}"

    except Exception as e:
        logger.error(f"üî• L·ªói: {e}")
        return f"L·ªói tra c·ª©u: {e}"
    finally:
        if page: await page.close()

if __name__ == "__main__":
    app.run()