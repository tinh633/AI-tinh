import asyncio
from mcp.server.fastmcp import FastMCP
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import urllib.parse
import logging
import sys

# 1. Fix l·ªói khi ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng chu·∫©n (VSCode/Jupyter)
if not hasattr(sys.stdout, 'buffer'):
    try:
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
    except Exception:
        pass

# C·∫•u h√¨nh log
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

app = FastMCP("traffic_law_mcp")

# ==============================================================================
# TOOL: Luat Lookup (S·ª≠ d·ª•ng Playwright)
# ==============================================================================
@app.tool()
async def luat_lookup(query: str) -> str:
    """
    Tra c·ª©u lu·∫≠t t·ª´ vbpl.vn s·ª≠ d·ª•ng tr√¨nh duy·ªát ·∫£o (Playwright).
    Gi√∫p v∆∞·ª£t qua c√°c l·ªói ch·∫∑n bot ho·∫∑c l·ªói t·∫£i trang tr·∫Øng.
    """
    log.info(f"üîç ƒêang t√¨m ki·∫øm: {query} (b·∫±ng Playwright)...")
    
    # M√£ h√≥a t·ª´ kh√≥a t√¨m ki·∫øm
    q = urllib.parse.quote(query)
    search_url = f"https://vbpl.vn/Pages/timkiemvbpl.aspx?Keyword={q}"

    async with async_playwright() as p:
        # Kh·ªüi t·∫°o tr√¨nh duy·ªát Chrome (ch·∫°y ·∫©n)
        try:
            browser = await p.chromium.launch(headless=False, slow_mo=1000)
        except Exception as e:
            return f"‚ùå L·ªói: Ch∆∞a c√†i tr√¨nh duy·ªát. H√£y ch·∫°y l·ªánh: 'python -m playwright install chromium'. Chi ti·∫øt: {e}"

        # T·∫°o ng·ªØ c·∫£nh gi·∫£ l·∫≠p tr√¨nh duy·ªát th·∫≠t
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        try:
            # --- B∆Ø·ªöC 1: T√åM KI·∫æM VƒÇN B·∫¢N ---
            log.info(f"Truy c·∫≠p: {search_url}")
            # Ch·ªù trang t·∫£i xong (domcontentloaded)
            await page.goto(search_url, timeout=30000, wait_until="domcontentloaded")
            
            # L·∫•y HTML ƒë√£ render
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # Logic l·ªçc link vƒÉn b·∫£n
            valid_links = []
            seen = set()
            
            # T√¨m c√°c th·∫ª <a> ch·ª©a link 'vbpq-toanvan.aspx'
            for a in soup.find_all("a", href=True):
                href = a.get("href")
                if "vbpq-toanvan.aspx" in href:
                    full_url = "https://vbpl.vn" + href if not href.startswith("http") else href
                    if full_url not in seen:
                        valid_links.append(full_url)
                        seen.add(full_url)
            
            # N·∫øu ch∆∞a t√¨m th·∫•y, th·ª≠ t√¨m theo class title (d·ª± ph√≤ng)
            if not valid_links:
                for a in soup.select("a.title"):
                    href = a.get("href")
                    if href:
                        full_url = "https://vbpl.vn" + href if not href.startswith("http") else href
                        if full_url not in seen:
                            valid_links.append(full_url)
                            seen.add(full_url)

            if not valid_links:
                return f"‚ùå Kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n n√†o cho '{query}' tr√™n VBPL."

            # --- B∆Ø·ªöC 2: ƒê·ªåC CHI TI·∫æT VƒÇN B·∫¢N ---
            # L·∫•y t·ªëi ƒëa 2 k·∫øt qu·∫£ ƒë·∫ßu ti√™n
            target_links = valid_links[:2]
            final_result = f"K·∫øt qu·∫£ t√¨m ki·∫øm VBPL (Playwright) cho '{query}':\n"

            for idx, link in enumerate(target_links, 1):
                log.info(f"ƒêang ƒë·ªçc chi ti·∫øt: {link}")
                try:
                    await page.goto(link, timeout=30000, wait_until="domcontentloaded")
                    
                    # Th·ª≠ ch·ªù n·ªôi dung ch√≠nh xu·∫•t hi·ªán (n·∫øu web load ch·∫≠m)
                    try:
                        await page.wait_for_selector("#toanvancontent", timeout=3000)
                    except:
                        pass # N·∫øu kh√¥ng c√≥ ID n√†y th√¨ c·ª© ƒë·ªçc ti·∫øp

                    sub_html = await page.content()
                    sub_soup = BeautifulSoup(sub_html, "html.parser")
                    
                    # C√°c selector ch·ª©a n·ªôi dung lu·∫≠t
                    body = (sub_soup.select_one("#toanvancontent") or 
                            sub_soup.select_one("#divContentDoc") or 
                            sub_soup.select_one("div.content-detail"))
                    
                    if body:
                        # X√≥a c√°c ph·∫ßn r√°c (script, qu·∫£ng c√°o, thanh c√¥ng c·ª•)
                        for tag in body(["script", "style", "div.minitoolbar", "div.ads"]):
                            tag.decompose()
                        
                        text = body.get_text(" ", strip=True)
                        text = " ".join(text.split()) # L√†m s·∫°ch kho·∫£ng tr·∫Øng th·ª´a
                        final_result += f"\n=== VƒÉn b·∫£n {idx}: {link} ===\n{text[:4000]}\n"
                    else:
                        final_result += f"\n=== VƒÉn b·∫£n {idx}: {link} (Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung) ===\n"

                except Exception as e:
                    final_result += f"\nL·ªói khi ƒë·ªçc link {link}: {e}\n"
            
            return final_result

        except Exception as e:
            return f"‚ùå L·ªói Playwright trong qu√° tr√¨nh x·ª≠ l√Ω: {e}"
        finally:
            # Lu√¥n ƒë√≥ng tr√¨nh duy·ªát ƒë·ªÉ gi·∫£i ph√≥ng RAM
            await browser.close()

if __name__ == "__main__":
    try:
        app.run()
    except Exception:
        pass