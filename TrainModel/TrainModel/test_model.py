# import torch
# import torch.nn as nn
# from torch.utils.data import Dataset, DataLoader
# import os
# import cv2
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report
# import shutil
# from datetime import datetime
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# import warnings
# warnings.filterwarnings('ignore')
# from ultralytics import YOLO
# import yaml
# from tqdm import tqdm
# import glob

# # ==================== CONFIGURATION ====================
# DATASET_PATHS = {
#     'open_eyes': '/home/gess/Documents/Data/Open_Eyes/',
#     'closed_eyes': '/home/gess/Documents/Data/Closed_Eyes/', 
#     'real_webcam': '/home/gess/Pictures/Webcam/',
#     'additional_webcam': '/home/gess/Documents/Data/Additional_Webcam/',
#     'videos': '/home/gess/Documents/Data/Fold1_part2/',
#     'new_videos': '/home/gess/Documents/Data/New_Videos/',
#     'yolo_dataset': '/home/gess/Documents/Data/YOLO_Dataset/',
#     'model_save': '/home/gess/Documents/sub/Py/hhehee/eye_detection_model/',
#     'output': '/home/gess/Documents/sub/Py/hhehee/eye_detection_results/'
# }

# # T·∫°o th∆∞ m·ª•c YOLO dataset structure
# YOLO_DIRS = ['images/train', 'images/val', 'labels/train', 'labels/val']
# for dir_name in YOLO_DIRS:
#     os.makedirs(os.path.join(DATASET_PATHS['yolo_dataset'], dir_name), exist_ok=True)

# for path in DATASET_PATHS.values():
#     if path.endswith('/'):
#         os.makedirs(path, exist_ok=True)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"üöÄ Using device: {device}")

# # ==================== YOLO DATA PROCESSOR ====================
# class YOLODataProcessor:
#     def __init__(self, img_size=640):
#         self.img_size = img_size
#         self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
#         self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        
#     def prepare_yolo_dataset(self):
#         """Chu·∫©n b·ªã dataset cho YOLO training"""
#         print("üîÑ Preparing YOLO dataset...")
        
#         all_images = []
#         all_labels = []
        
#         # 1. Process basic images
#         print("üìÅ Processing basic images...")
#         all_images.extend(self._process_basic_images())
        
#         # 2. Process webcam images
#         print("üì∏ Processing webcam images...")
#         all_images.extend(self._process_webcam_images())
        
#         # 3. Process videos
#         print("üé• Processing videos...")
#         all_images.extend(self._process_videos())
        
#         # Split data v√† t·∫°o YOLO format
#         self._create_yolo_dataset(all_images)
        
#     def _process_basic_images(self):
#         """X·ª≠ l√Ω ·∫£nh c∆° b·∫£n"""
#         images_info = []
        
#         # Open eyes
#         open_files = glob.glob(os.path.join(DATASET_PATHS['open_eyes'], '*.jpg')) + \
#                     glob.glob(os.path.join(DATASET_PATHS['open_eyes'], '*.png'))
        
#         for img_path in open_files[:1000]:  # Gi·ªõi h·∫°n 1000 ·∫£nh m·ªói class
#             images_info.append({
#                 'path': img_path,
#                 'label': 0,  # 0 = open eyes
#                 'source': 'basic_open'
#             })
        
#         # Closed eyes
#         closed_files = glob.glob(os.path.join(DATASET_PATHS['closed_eyes'], '*.jpg')) + \
#                       glob.glob(os.path.join(DATASET_PATHS['closed_eyes'], '*.png'))
        
#         for img_path in closed_files[:1000]:
#             images_info.append({
#                 'path': img_path,
#                 'label': 1,  # 1 = closed eyes
#                 'source': 'basic_closed'
#             })
        
#         return images_info
    
#     def _process_webcam_images(self):
#         """X·ª≠ l√Ω ·∫£nh webcam"""
#         images_info = []
#         webcam_folders = [DATASET_PATHS['real_webcam'], DATASET_PATHS['additional_webcam']]
        
#         for folder in webcam_folders:
#             if not os.path.exists(folder):
#                 continue
                
#             for img_path in glob.glob(os.path.join(folder, '*.jpg')) + glob.glob(os.path.join(folder, '*.png')):
#                 filename = os.path.basename(img_path).lower()
                
#                 if any(keyword in filename for keyword in ['open', 'mo', 'o_']):
#                     label = 0
#                 elif any(keyword in filename for keyword in ['closed', 'dong', 'c_', 'nhammat']):
#                     label = 1
#                 else:
#                     continue
                
#                 images_info.append({
#                     'path': img_path,
#                     'label': label,
#                     'source': 'webcam'
#                 })
        
#         return images_info
    
#     def _process_videos(self):
#         """X·ª≠ l√Ω video - tr√≠ch xu·∫•t frames"""
#         images_info = []
#         video_folders = [DATASET_PATHS['videos'], DATASET_PATHS['new_videos']]
        
#         for folder in video_folders:
#             if not os.path.exists(folder):
#                 continue
                
#             for video_path in glob.glob(os.path.join(folder, '*.mp4')) + \
#                              glob.glob(os.path.join(folder, '*.avi')) + \
#                              glob.glob(os.path.join(folder, '*.mov')):
                
#                 # X√°c ƒë·ªãnh label t·ª´ t√™n video
#                 video_name = os.path.basename(video_path).lower()
#                 if any(keyword in video_name for keyword in ['open', 'mo']):
#                     label = 0
#                 elif any(keyword in video_name for keyword in ['closed', 'dong', 'nhammat']):
#                     label = 1
#                 else:
#                     continue
                
#                 # Tr√≠ch xu·∫•t frames
#                 frames = self._extract_frames_from_video(video_path, max_frames=20)
#                 for i, frame_path in enumerate(frames):
#                     images_info.append({
#                         'path': frame_path,
#                         'label': label,
#                         'source': 'video'
#                     })
        
#         return images_info
    
#     def _extract_frames_from_video(self, video_path, max_frames=20):
#         """Tr√≠ch xu·∫•t frames t·ª´ video"""
#         frames = []
#         cap = cv2.VideoCapture(video_path)
        
#         if not cap.isOpened():
#             return frames
        
#         fps = cap.get(cv2.CAP_PROP_FPS)
#         frame_interval = max(1, int(fps))  # 1 frame m·ªói gi√¢y
        
#         frame_count = 0
#         saved_count = 0
        
#         while saved_count < max_frames:
#             ret, frame = cap.read()
#             if not ret:
#                 break
                
#             if frame_count % frame_interval == 0:
#                 # L∆∞u frame t·∫°m th·ªùi
#                 frame_filename = f"temp_frame_{os.path.basename(video_path)}_{saved_count}.jpg"
#                 frame_path = os.path.join('/tmp', frame_filename)
#                 cv2.imwrite(frame_path, frame)
#                 frames.append(frame_path)
#                 saved_count += 1
            
#             frame_count += 1
        
#         cap.release()
#         return frames
    
#     def _create_yolo_dataset(self, images_info):
#         """T·∫°o dataset format YOLO"""
#         print("üì¶ Creating YOLO dataset format...")
        
#         if len(images_info) == 0:
#             print("‚ùå No images found for dataset creation!")
#             return
        
#         # Split data
#         train_data, val_data = train_test_split(
#             images_info, test_size=0.2, random_state=42, 
#             stratify=[img['label'] for img in images_info]
#         )
        
#         # T·∫°o YOLO format cho train v√† val
#         self._create_yolo_split(train_data, 'train')
#         self._create_yolo_split(val_data, 'val')
        
#         # T·∫°o file data.yaml
#         self._create_yaml_config()
        
#         print(f"‚úÖ YOLO dataset created! Train: {len(train_data)}, Val: {len(val_data)}")
    
#     def _create_yolo_split(self, data, split_type):
#         """T·∫°o d·ªØ li·ªáu cho train/val split"""
#         image_dir = os.path.join(DATASET_PATHS['yolo_dataset'], 'images', split_type)
#         label_dir = os.path.join(DATASET_PATHS['yolo_dataset'], 'labels', split_type)
        
#         for i, img_info in enumerate(tqdm(data, desc=f"Processing {split_type}")):
#             try:
#                 # ƒê·ªçc v√† x·ª≠ l√Ω ·∫£nh
#                 img = cv2.imread(img_info['path'])
#                 if img is None:
#                     continue
                
#                 # Resize ·∫£nh
#                 img_resized = cv2.resize(img, (self.img_size, self.img_size))
                
#                 # L∆∞u ·∫£nh
#                 img_filename = f"{split_type}_{i:06d}.jpg"
#                 img_save_path = os.path.join(image_dir, img_filename)
#                 cv2.imwrite(img_save_path, img_resized)
                
#                 # T·∫°o label file (YOLO format)
#                 label_filename = f"{split_type}_{i:06d}.txt"
#                 label_save_path = os.path.join(label_dir, label_filename)
                
#                 # YOLO format: class x_center y_center width height (normalized)
#                 # V·ªõi ·∫£nh eye crop, coi nh∆∞ to√†n b·ªô ·∫£nh l√† bounding box
#                 with open(label_save_path, 'w') as f:
#                     # class_id, x_center, y_center, width, height (all normalized)
#                     f.write(f"{img_info['label']} 0.5 0.5 1.0 1.0\n")
                    
#             except Exception as e:
#                 print(f"‚ùå Error processing {img_info['path']}: {e}")
#                 continue
    
#     def _create_yaml_config(self):
#         """T·∫°o file c·∫•u h√¨nh YOLO"""
#         yaml_content = {
#             'path': DATASET_PATHS['yolo_dataset'],
#             'train': 'images/train',
#             'val': 'images/val',
#             'nc': 2,  # number of classes
#             'names': ['open_eye', 'closed_eye']  # class names
#         }
        
#         yaml_path = os.path.join(DATASET_PATHS['yolo_dataset'], 'data.yaml')
#         with open(yaml_path, 'w') as f:
#             yaml.dump(yaml_content, f, default_flow_style=False)
        
#         print(f"‚úÖ YAML config created: {yaml_path}")

# # ==================== YOLO MODEL TRAINER ====================
# class YOLOEyeTrainer:
#     def __init__(self, model_size='n'):  # n, s, m, l, x
#         self.model_size = model_size
#         self.model = None
#         self.data_processor = YOLODataProcessor()
        
#     def prepare_data(self):
#         """Chu·∫©n b·ªã d·ªØ li·ªáu cho training"""
#         print("üîÑ Preparing training data...")
#         self.data_processor.prepare_yolo_dataset()
        
#     def train_model(self, epochs=100, imgsz=640, batch_size=16):
#         """Training YOLO model"""
#         print("üöÄ Starting YOLO Training...")
        
#         # Chu·∫©n b·ªã d·ªØ li·ªáu
#         self.prepare_data()
        
#         # Load YOLO model
#         model_name = f'yolov8{self.model_size}.pt'
#         self.model = YOLO(model_name)
        
#         # Training configuration
#         training_args = {
#             'data': os.path.join(DATASET_PATHS['yolo_dataset'], 'data.yaml'),
#             'epochs': epochs,
#             'imgsz': imgsz,
#             'batch': batch_size,
#             'patience': 20,
#             'save': True,
#             'exist_ok': True,
#             'pretrained': True,
#             'optimizer': 'AdamW',
#             'lr0': 0.001,
#             'weight_decay': 0.0005,
#             'device': '0' if torch.cuda.is_available() else 'cpu',
#             'workers': 4,
#             'project': DATASET_PATHS['model_save'],
#             'name': f'yolov8{self.model_size}_eye_detection',
#             'verbose': True
#         }
        
#         print(f"üìä Training Configuration:")
#         print(f"   Model: YOLOv8{self.model_size}")
#         print(f"   Epochs: {epochs}")
#         print(f"   Image size: {imgsz}")
#         print(f"   Batch size: {batch_size}")
#         print(f"   Device: {training_args['device']}")
        
#         # Start training
#         results = self.model.train(**training_args)
        
#         # Save best model
#         self._save_best_model()
        
#         return results
    
#     def _save_best_model(self):
#         """L∆∞u best model v√† convert sang format ph√π h·ª£p"""
#         # Model s·∫Ω t·ª± ƒë·ªông l∆∞u trong th∆∞ m·ª•c runs
#         # Copy best model ƒë·∫øn th∆∞ m·ª•c model_save
#         best_model_path = self.model.ckpt_path
        
#         if best_model_path and os.path.exists(best_model_path):
#             # Copy best model
#             final_model_path = os.path.join(
#                 DATASET_PATHS['model_save'], 
#                 'best_eye_detection_yolo.pt'
#             )
#             shutil.copy2(best_model_path, final_model_path)
#             print(f"‚úÖ Best model saved: {final_model_path}")
        
#     def evaluate_model(self):
#         """ƒê√°nh gi√° model - ƒê√É S·ª¨A L·ªñI"""
#         if self.model is None:
#             print("‚ùå No model available for evaluation!")
#             return
        
#         # Validation dataset path
#         val_data_path = os.path.join(DATASET_PATHS['yolo_dataset'], 'data.yaml')
        
#         # Evaluate
#         metrics = self.model.val(data=val_data_path)
        
#         print(f"üìä Model Evaluation Results:")
#         print(f"   mAP50: {metrics.box.map50:.4f}")
#         print(f"   mAP50-95: {metrics.box.map:.4f}")
        
#         # S·ª¨A L·ªñI: S·ª≠ d·ª•ng attributes ƒë√∫ng t·ª´ metrics
#         if hasattr(metrics, 'speed'):
#             print(f"   Inference Speed: {metrics.speed['inference']:.1f}ms/img")
        
#         # In k·∫øt qu·∫£ chi ti·∫øt cho t·ª´ng class
#         if hasattr(metrics, 'results_dict'):
#             results_dict = metrics.results_dict
#             print(f"   Precision: {results_dict.get('metrics/precision(B)', 0):.4f}")
#             print(f"   Recall: {results_dict.get('metrics/recall(B)', 0):.4f}")
        
#         return metrics
    
#     def export_for_web(self, format='torchscript'):
#         """Export model cho web deployment"""
#         if self.model is None:
#             print("‚ùå No model available for export!")
#             return
        
#         # Load best model ƒë·ªÉ export
#         best_model_path = os.path.join(DATASET_PATHS['model_save'], 'best_eye_detection_yolo.pt')
#         if not os.path.exists(best_model_path):
#             print("‚ùå Best model not found for export!")
#             return
            
#         model_for_export = YOLO(best_model_path)
        
#         try:
#             if format == 'torchscript':
#                 exported_path = model_for_export.export(format='torchscript')
#             elif format == 'onnx':
#                 exported_path = model_for_export.export(format='onnx')
#             else:
#                 exported_path = model_for_export.export(format='pt')  # PyTorch
            
#             # Copy ƒë·∫øn v·ªã tr√≠ cu·ªëi c√πng
#             final_export_path = os.path.join(DATASET_PATHS['model_save'], f'eye_detection_web.{format}')
#             shutil.copy2(exported_path, final_export_path)
            
#             print(f"‚úÖ Model exported for web: {final_export_path}")
#             return final_export_path
            
#         except Exception as e:
#             print(f"‚ùå Export failed: {e}")
#             return None

# # ==================== REAL-TIME TESTING ====================
# class RealTimeTester:
#     def __init__(self, model_path):
#         self.model = YOLO(model_path)
#         self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
#     def test_webcam(self):
#         """Test real-time v·ªõi webcam"""
#         print("üé• Starting real-time webcam test...")
        
#         cap = cv2.VideoCapture(0)
#         if not cap.isOpened():
#             print("‚ùå Cannot open webcam!")
#             return
        
#         while True:
#             ret, frame = cap.read()
#             if not ret:
#                 break
            
#             # Detect faces
#             gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#             faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
            
#             for (x, y, w, h) in faces:
#                 # Extract face ROI
#                 face_roi = frame[y:y+h, x:x+w]
                
#                 # Run YOLO detection on face ROI
#                 results = self.model(face_roi, verbose=False)
                
#                 for result in results:
#                     boxes = result.boxes
#                     if boxes is not None:
#                         for box in boxes:
#                             cls = int(box.cls[0])
#                             conf = float(box.conf[0])
                            
#                             if conf > 0.5:  # Confidence threshold
#                                 label = "OPEN" if cls == 0 else "CLOSED"
#                                 color = (0, 255, 0) if cls == 0 else (0, 0, 255)
                                
#                                 # Draw on original frame
#                                 cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
#                                 cv2.putText(frame, f'{label} {conf:.2f}', 
#                                           (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 
#                                           0.7, color, 2)
            
#             cv2.imshow('Real-time Eye Detection - YOLO', frame)
            
#             if cv2.waitKey(1) & 0xFF == ord('q'):
#                 break
        
#         cap.release()
#         cv2.destroyAllWindows()

# # ==================== MAIN ====================
# def main():
#     print("üëÅÔ∏è YOLO EYE DETECTION TRAINING")
#     print("=" * 60)
#     print("üéØ Training YOLO model for eye open/closed detection")
#     print("üíæ Will export .pt file for web deployment")
#     print("=" * 60)
    
#     try:
#         # Kh·ªüi t·∫°o trainer
#         trainer = YOLOEyeTrainer(model_size='n')  # n = nano (nh·ªè, nhanh)
        
#         # Training
#         results = trainer.train_model(epochs=100, batch_size=16)
        
#         # ƒê√°nh gi√°
#         trainer.evaluate_model()
        
#         # Export cho web
#         trainer.export_for_web(format='torchscript')  # Ho·∫∑c 'onnx', 'pt'
        
#         print(f"\nüéä YOLO TRAINING COMPLETED!")
#         print(f"üìÅ Model saved in: {DATASET_PATHS['model_save']}")
#         print("üöÄ Ready for web deployment!")
        
#     except Exception as e:
#         print(f"‚ùå Training failed: {e}")
#         import traceback
#         traceback.print_exc()

# def test_real_time():
#     """Test real-time detection"""
#     model_path = os.path.join(DATASET_PATHS['model_save'], 'best_eye_detection_yolo.pt')
    
#     if os.path.exists(model_path):
#         tester = RealTimeTester(model_path)
#         tester.test_webcam()
#     else:
#         print("‚ùå Model not found! Please train first.")

# if __name__ == "__main__":
#     main()
    
#     # Uncomment ƒë·ªÉ test real-time sau khi training
#     # test_real_time()


import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import shutil
from datetime import datetime
import albumentations as A
from albumentations.pytorch import ToTensorV2
import warnings
warnings.filterwarnings('ignore')
from ultralytics import YOLO
import yaml
from tqdm import tqdm
import glob

# ==================== CONFIGURATION ====================
DATASET_PATHS = {
    'open_eyes': '/home/gess/Documents/Data/Open_Eyes/',
    'closed_eyes': '/home/gess/Documents/Data/Closed_Eyes/', 
    'real_webcam': '/home/gess/Pictures/Webcam/',
    'additional_webcam': '/home/gess/Documents/Data/Additional_Webcam/',
    'videos': '/home/gess/Documents/Data/Fold1_part2/',
    'new_videos': '/home/gess/Documents/Data/New_Videos/',
    'yolo_dataset': '/home/gess/Documents/Data/YOLO_Dataset/',
    'model_save': '/home/gess/Documents/sub/Py/hhehee/eye_detection_model/',
    'output': '/home/gess/Documents/sub/Py/hhehee/eye_detection_results/'
}

# T·∫°o th∆∞ m·ª•c YOLO dataset structure
YOLO_DIRS = ['images/train', 'images/val', 'labels/train', 'labels/val']
for dir_name in YOLO_DIRS:
    os.makedirs(os.path.join(DATASET_PATHS['yolo_dataset'], dir_name), exist_ok=True)

for path in DATASET_PATHS.values():
    if path.endswith('/'):
        os.makedirs(path, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Using device: {device}")

# ==================== YOLO DATA PROCESSOR ====================
class YOLODataProcessor:
    def __init__(self, img_size=640):
        self.img_size = img_size
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        
    def prepare_yolo_dataset(self):
        """Chu·∫©n b·ªã dataset cho YOLO training"""
        print("üîÑ Preparing YOLO dataset...")
        
        all_images = []
        all_labels = []
        
        # 1. Process basic images
        print("üìÅ Processing basic images...")
        all_images.extend(self._process_basic_images())
        
        # 2. Process webcam images
        print("üì∏ Processing webcam images...")
        all_images.extend(self._process_webcam_images())
        
        # 3. Process videos
        print("üé• Processing videos...")
        all_images.extend(self._process_videos())
        
        # Split data v√† t·∫°o YOLO format
        self._create_yolo_dataset(all_images)
        
    def _process_basic_images(self):
        """X·ª≠ l√Ω ·∫£nh c∆° b·∫£n"""
        images_info = []
        
        # Open eyes
        open_files = glob.glob(os.path.join(DATASET_PATHS['open_eyes'], '*.jpg')) + \
                    glob.glob(os.path.join(DATASET_PATHS['open_eyes'], '*.png'))
        
        for img_path in open_files[:1000]:  # Gi·ªõi h·∫°n 1000 ·∫£nh m·ªói class
            images_info.append({
                'path': img_path,
                'label': 0,  # 0 = open eyes
                'source': 'basic_open'
            })
        
        # Closed eyes
        closed_files = glob.glob(os.path.join(DATASET_PATHS['closed_eyes'], '*.jpg')) + \
                      glob.glob(os.path.join(DATASET_PATHS['closed_eyes'], '*.png'))
        
        for img_path in closed_files[:1000]:
            images_info.append({
                'path': img_path,
                'label': 1,  # 1 = closed eyes
                'source': 'basic_closed'
            })
        
        return images_info
    
    def _process_webcam_images(self):
        """X·ª≠ l√Ω ·∫£nh webcam"""
        images_info = []
        webcam_folders = [DATASET_PATHS['real_webcam'], DATASET_PATHS['additional_webcam']]
        
        for folder in webcam_folders:
            if not os.path.exists(folder):
                continue
                
            for img_path in glob.glob(os.path.join(folder, '*.jpg')) + glob.glob(os.path.join(folder, '*.png')):
                filename = os.path.basename(img_path).lower()
                
                if any(keyword in filename for keyword in ['open', 'mo', 'o_']):
                    label = 0
                elif any(keyword in filename for keyword in ['closed', 'dong', 'c_', 'nhammat']):
                    label = 1
                else:
                    continue
                
                images_info.append({
                    'path': img_path,
                    'label': label,
                    'source': 'webcam'
                })
        
        return images_info
    
    def _process_videos(self):
        """X·ª≠ l√Ω video - tr√≠ch xu·∫•t frames"""
        images_info = []
        video_folders = [DATASET_PATHS['videos'], DATASET_PATHS['new_videos']]
        
        for folder in video_folders:
            if not os.path.exists(folder):
                continue
                
            for video_path in glob.glob(os.path.join(folder, '*.mp4')) + \
                             glob.glob(os.path.join(folder, '*.avi')) + \
                             glob.glob(os.path.join(folder, '*.mov')):
                
                # X√°c ƒë·ªãnh label t·ª´ t√™n video
                video_name = os.path.basename(video_path).lower()
                if any(keyword in video_name for keyword in ['open', 'mo']):
                    label = 0
                elif any(keyword in video_name for keyword in ['closed', 'dong', 'nhammat']):
                    label = 1
                else:
                    continue
                
                # Tr√≠ch xu·∫•t frames
                frames = self._extract_frames_from_video(video_path, max_frames=20)
                for i, frame_path in enumerate(frames):
                    images_info.append({
                        'path': frame_path,
                        'label': label,
                        'source': 'video'
                    })
        
        return images_info
    
    def _extract_frames_from_video(self, video_path, max_frames=20):
        """Tr√≠ch xu·∫•t frames t·ª´ video"""
        frames = []
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return frames
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = max(1, int(fps))  # 1 frame m·ªói gi√¢y
        
        frame_count = 0
        saved_count = 0
        
        while saved_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % frame_interval == 0:
                # L∆∞u frame t·∫°m th·ªùi
                frame_filename = f"temp_frame_{os.path.basename(video_path)}_{saved_count}.jpg"
                frame_path = os.path.join('/tmp', frame_filename)
                cv2.imwrite(frame_path, frame)
                frames.append(frame_path)
                saved_count += 1
            
            frame_count += 1
        
        cap.release()
        return frames
    
    def _create_yolo_dataset(self, images_info):
        """T·∫°o dataset format YOLO"""
        print("üì¶ Creating YOLO dataset format...")
        
        if len(images_info) == 0:
            print("‚ùå No images found for dataset creation!")
            return
        
        # Split data
        train_data, val_data = train_test_split(
            images_info, test_size=0.2, random_state=42, 
            stratify=[img['label'] for img in images_info]
        )
        
        # T·∫°o YOLO format cho train v√† val
        self._create_yolo_split(train_data, 'train')
        self._create_yolo_split(val_data, 'val')
        
        # T·∫°o file data.yaml
        self._create_yaml_config()
        
        print(f"‚úÖ YOLO dataset created! Train: {len(train_data)}, Val: {len(val_data)}")
    
    def _create_yolo_split(self, data, split_type):
        """T·∫°o d·ªØ li·ªáu cho train/val split"""
        image_dir = os.path.join(DATASET_PATHS['yolo_dataset'], 'images', split_type)
        label_dir = os.path.join(DATASET_PATHS['yolo_dataset'], 'labels', split_type)
        
        for i, img_info in enumerate(tqdm(data, desc=f"Processing {split_type}")):
            try:
                # ƒê·ªçc v√† x·ª≠ l√Ω ·∫£nh
                img = cv2.imread(img_info['path'])
                if img is None:
                    continue
                
                # Resize ·∫£nh
                img_resized = cv2.resize(img, (self.img_size, self.img_size))
                
                # L∆∞u ·∫£nh
                img_filename = f"{split_type}_{i:06d}.jpg"
                img_save_path = os.path.join(image_dir, img_filename)
                cv2.imwrite(img_save_path, img_resized)
                
                # T·∫°o label file (YOLO format)
                label_filename = f"{split_type}_{i:06d}.txt"
                label_save_path = os.path.join(label_dir, label_filename)
                
                # YOLO format: class x_center y_center width height (normalized)
                # V·ªõi ·∫£nh eye crop, coi nh∆∞ to√†n b·ªô ·∫£nh l√† bounding box
                with open(label_save_path, 'w') as f:
                    # class_id, x_center, y_center, width, height (all normalized)
                    f.write(f"{img_info['label']} 0.5 0.5 1.0 1.0\n")
                    
            except Exception as e:
                print(f"‚ùå Error processing {img_info['path']}: {e}")
                continue
    
    def _create_yaml_config(self):
        """T·∫°o file c·∫•u h√¨nh YOLO"""
        yaml_content = {
            'path': DATASET_PATHS['yolo_dataset'],
            'train': 'images/train',
            'val': 'images/val',
            'nc': 2,  # number of classes
            'names': ['open_eye', 'closed_eye']  # class names
        }
        
        yaml_path = os.path.join(DATASET_PATHS['yolo_dataset'], 'data.yaml')
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f, default_flow_style=False)
        
        print(f"‚úÖ YAML config created: {yaml_path}")

# ==================== YOLO MODEL TRAINER ====================
class YOLOEyeTrainer:
    def __init__(self, model_size='n'):  # n, s, m, l, x
        self.model_size = model_size
        self.model = None
        self.data_processor = YOLODataProcessor()
        
    def prepare_data(self):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho training"""
        print("üîÑ Preparing training data...")
        self.data_processor.prepare_yolo_dataset()
        
    def train_model(self, epochs=100, imgsz=640, batch_size=16):
        """Training YOLO model"""
        print("üöÄ Starting YOLO Training...")
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        self.prepare_data()
        
        # Load YOLO model
        model_name = f'yolov8{self.model_size}.pt'
        self.model = YOLO(model_name)
        
        # Training configuration
        training_args = {
            'data': os.path.join(DATASET_PATHS['yolo_dataset'], 'data.yaml'),
            'epochs': epochs,
            'imgsz': imgsz,
            'batch': batch_size,
            'patience': 20,
            'save': True,
            'exist_ok': True,
            'pretrained': True,
            'optimizer': 'AdamW',
            'lr0': 0.001,
            'weight_decay': 0.0005,
            'device': '0' if torch.cuda.is_available() else 'cpu',
            'workers': 4,
            'project': DATASET_PATHS['model_save'],
            'name': f'yolov8{self.model_size}_eye_detection',
            'verbose': True
        }
        
        print(f"üìä Training Configuration:")
        print(f"   Model: YOLOv8{self.model_size}")
        print(f"   Epochs: {epochs}")
        print(f"   Image size: {imgsz}")
        print(f"   Batch size: {batch_size}")
        print(f"   Device: {training_args['device']}")
        
        # Start training
        results = self.model.train(**training_args)
        
        # Save best model
        self._save_best_model()
        
        return results
    
    def _save_best_model(self):
        """L∆∞u best model v√† convert sang format ph√π h·ª£p"""
        # Model s·∫Ω t·ª± ƒë·ªông l∆∞u trong th∆∞ m·ª•c runs
        # Copy best model ƒë·∫øn th∆∞ m·ª•c model_save
        best_model_path = self.model.ckpt_path
        
        if best_model_path and os.path.exists(best_model_path):
            # Copy best model
            final_model_path = os.path.join(
                DATASET_PATHS['model_save'], 
                'best_eye_detection_yolo.pt'
            )
            shutil.copy2(best_model_path, final_model_path)
            print(f"‚úÖ Best model saved: {final_model_path}")
        
    def evaluate_model(self):
        """ƒê√°nh gi√° model - ƒê√É S·ª¨A L·ªñI"""
        if self.model is None:
            print("‚ùå No model available for evaluation!")
            return
        
        # Validation dataset path
        val_data_path = os.path.join(DATASET_PATHS['yolo_dataset'], 'data.yaml')
        
        # Evaluate
        metrics = self.model.val(data=val_data_path)
        
        print(f"üìä Model Evaluation Results:")
        print(f"   mAP50: {metrics.box.map50:.4f}")
        print(f"   mAP50-95: {metrics.box.map:.4f}")
        
        # S·ª¨A L·ªñI: S·ª≠ d·ª•ng attributes ƒë√∫ng t·ª´ metrics
        if hasattr(metrics, 'speed'):
            print(f"   Inference Speed: {metrics.speed['inference']:.1f}ms/img")
        
        # In k·∫øt qu·∫£ chi ti·∫øt cho t·ª´ng class
        if hasattr(metrics, 'results_dict'):
            results_dict = metrics.results_dict
            print(f"   Precision: {results_dict.get('metrics/precision(B)', 0):.4f}")
            print(f"   Recall: {results_dict.get('metrics/recall(B)', 0):.4f}")
        
        return metrics
    
    def export_for_web(self, format='torchscript'):
        """Export model cho web deployment"""
        if self.model is None:
            print("‚ùå No model available for export!")
            return
        
        # Load best model ƒë·ªÉ export
        best_model_path = os.path.join(DATASET_PATHS['model_save'], 'best_eye_detection_yolo.pt')
        if not os.path.exists(best_model_path):
            print("‚ùå Best model not found for export!")
            return
            
        model_for_export = YOLO(best_model_path)
        
        try:
            if format == 'torchscript':
                exported_path = model_for_export.export(format='torchscript')
            elif format == 'onnx':
                exported_path = model_for_export.export(format='onnx')
            else:
                exported_path = model_for_export.export(format='pt')  # PyTorch
            
            # Copy ƒë·∫øn v·ªã tr√≠ cu·ªëi c√πng
            final_export_path = os.path.join(DATASET_PATHS['model_save'], f'eye_detection_web.{format}')
            shutil.copy2(exported_path, final_export_path)
            
            print(f"‚úÖ Model exported for web: {final_export_path}")
            return final_export_path
            
        except Exception as e:
            print(f"‚ùå Export failed: {e}")
            return None

# ==================== REAL-TIME TESTING ====================
class RealTimeTester:
    def __init__(self, model_path):
        self.model = YOLO(model_path)
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
    def test_webcam(self):
        """Test real-time v·ªõi webcam"""
        print("üé• Starting real-time webcam test...")
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå Cannot open webcam!")
            return
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Detect faces
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
            
            for (x, y, w, h) in faces:
                # Extract face ROI
                face_roi = frame[y:y+h, x:x+w]
                
                # Run YOLO detection on face ROI
                results = self.model(face_roi, verbose=False)
                
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            cls = int(box.cls[0])
                            conf = float(box.conf[0])
                            
                            if conf > 0.5:  # Confidence threshold
                                label = "OPEN" if cls == 0 else "CLOSED"
                                color = (0, 255, 0) if cls == 0 else (0, 0, 255)
                                
                                # Draw on original frame
                                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                                cv2.putText(frame, f'{label} {conf:.2f}', 
                                          (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 
                                          0.7, color, 2)
            
            cv2.imshow('Real-time Eye Detection - YOLO', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

# ==================== MAIN ====================
def main():
    print("üëÅÔ∏è YOLO EYE DETECTION TRAINING")
    print("=" * 60)
    print("üéØ Training YOLO model for eye open/closed detection")
    print("üíæ Will export .pt file for web deployment")
    print("=" * 60)
    
    try:
        # Kh·ªüi t·∫°o trainer
        trainer = YOLOEyeTrainer(model_size='n')  # n = nano (nh·ªè, nhanh)
        
        # Training
        results = trainer.train_model(epochs=100, batch_size=16)
        
        # ƒê√°nh gi√°
        trainer.evaluate_model()
        
        # Export cho web
        trainer.export_for_web(format='torchscript')  # Ho·∫∑c 'onnx', 'pt'
        
        print(f"\nüéä YOLO TRAINING COMPLETED!")
        print(f"üìÅ Model saved in: {DATASET_PATHS['model_save']}")
        print("üöÄ Ready for web deployment!")
        
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()

def test_real_time():
    """Test real-time detection"""
    model_path = os.path.join(DATASET_PATHS['model_save'], 'best_eye_detection_yolo.pt')
    
    if os.path.exists(model_path):
        tester = RealTimeTester(model_path)
        tester.test_webcam()
    else:
        print("‚ùå Model not found! Please train first.")

if __name__ == "__main__":
    main()
    
    # Uncomment ƒë·ªÉ test real-time sau khi training
    # test_real_time()