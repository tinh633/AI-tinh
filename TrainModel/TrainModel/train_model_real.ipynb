{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5dc1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "try:\n",
    "    import mediapipe as mp # tu·ª≥ ch·ªçn ƒë·ªÉ t·∫°o bbox m·∫Øt/mi·ªáng chu·∫©n h∆°n\n",
    "    _HAS_MEDIAPIPE = True\n",
    "except Exception:\n",
    "    _HAS_MEDIAPIPE = False\n",
    "\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc0ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------- Utils ---------------------------- #\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def clean_tmp(tmp_dir: Path):\n",
    "    if tmp_dir.exists():\n",
    "        for f in tmp_dir.glob('*'):\n",
    "            try:\n",
    "                f.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "def natural_key(s: str):\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d4845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Config ---------------------------- #\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    root: Path\n",
    "    open_eyes: Path\n",
    "    closed_eyes: Path\n",
    "    yawns: Path\n",
    "    videos: Path\n",
    "    new_videos: Path\n",
    "    yolo_dataset: Path\n",
    "    output: Path\n",
    "    model_dir: Path\n",
    "    tmp: Path\n",
    "\n",
    "    @staticmethod\n",
    "    def from_env() -> 'Paths':\n",
    "        load_dotenv(override=True)\n",
    "        root = Path(os.getenv('DATA_ROOT', './data')).resolve()\n",
    "        return Paths(\n",
    "            root=root,\n",
    "            open_eyes=Path(os.getenv('OPEN_EYES', root/'Open_Eyes')).resolve(),\n",
    "            closed_eyes=Path(os.getenv('CLOSED_EYES', root/'Closed_Eyes')).resolve(),\n",
    "            yawns=Path(os.getenv('YAWNS', root/'Yawns')).resolve(),\n",
    "            videos=Path(os.getenv('VIDEOS', root/'Videos')).resolve(),\n",
    "            new_videos=Path(os.getenv('NEW_VIDEOS', root/'New_Videos')).resolve(),\n",
    "            yolo_dataset=Path(os.getenv('YOLO_DATASET', root/'YOLO_Dataset')).resolve(),\n",
    "            output=Path(os.getenv('OUTPUT', './outputs')).resolve(),\n",
    "            model_dir=Path(os.getenv('MODEL_DIR', './models')).resolve(),\n",
    "            tmp=Path(os.getenv('TMP_DIR', './tmp')).resolve(),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e5fd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------- Label helpers ---------------------------- #\n",
    "\n",
    "CLASS_MAP = {\n",
    "    'open_eye': 0,\n",
    "    'closed_eye': 1,\n",
    "    # c√≥ th·ªÉ m·ªü r·ªông: 'yawn': 2\n",
    "}\n",
    "\n",
    "CLASS_NAMES = ['open_eye', 'closed_eye']\n",
    "\n",
    "\n",
    "def write_yolo_label(label_path: Path, cls_id: int, xyxy: Tuple[int,int,int,int], img_w: int, img_h: int):\n",
    "    x1, y1, x2, y2 = xyxy\n",
    "    # clip\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(img_w-1, x2), min(img_h-1, y2)\n",
    "    # convert to xywh normalized\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    if w <= 1 or h <= 1:\n",
    "        return False\n",
    "    cx = x1 + w/2\n",
    "    cy = y1 + h/2\n",
    "    nx = cx / img_w\n",
    "    ny = cy / img_h\n",
    "    nw = w / img_w\n",
    "    nh = h / img_h\n",
    "    label_path.write_text(f\"{cls_id} {nx:.6f} {ny:.6f} {nw:.6f} {nh:.6f}\\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f94c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Detectors (for auto-label) ---------------------------- #\n",
    "\n",
    "class ROIExtractor:\n",
    "    \"\"\"T·∫°o bbox m·∫Øt/mi·ªáng t·ª´ ·∫£nh khu√¥n m·∫∑t. ∆Øu ti√™n MediaPipe (·ªïn ƒë·ªãnh), fallback HaarCascade.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "        if _HAS_MEDIAPIPE:\n",
    "            self.mp_face = mp.solutions.face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True)\n",
    "        else:\n",
    "            self.mp_face = None\n",
    "\n",
    "    def infer_eye_boxes(self, img: np.ndarray) -> List[Tuple[int,int,int,int]]:\n",
    "        h, w = img.shape[:2]\n",
    "        # --- MediaPipe path ---\n",
    "        if self.mp_face is not None:\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            res = self.mp_face.process(img_rgb)\n",
    "            if res.multi_face_landmarks:\n",
    "                boxes = []\n",
    "                for lm in res.multi_face_landmarks:\n",
    "                    # indices cho m·∫Øt tr√°i/ph·∫£i (FaceMesh mesh indices)\n",
    "                    left_ids = [33, 133, 159, 145]\n",
    "                    right_ids = [362, 263, 386, 374]\n",
    "                    for ids in (left_ids, right_ids):\n",
    "                        xs = [int(lm.landmark[i].x * w) for i in ids]\n",
    "                        ys = [int(lm.landmark[i].y * h) for i in ids]\n",
    "                        x1, y1, x2, y2 = min(xs), min(ys), max(xs), max(ys)\n",
    "                        # m·ªü r·ªông nh·∫π ƒë·ªÉ ch·ª©a vi·ªÅn\n",
    "                        pad = int(0.25 * max(x2-x1, y2-y1))\n",
    "                        x1, y1 = max(0, x1 - pad), max(0, y1 - pad)\n",
    "                        x2, y2 = min(w-1, x2 + pad), min(h-1, y2 + pad)\n",
    "                        boxes.append((x1, y1, x2, y2))\n",
    "                return boxes\n",
    "        # --- Haar fallback ---\n",
    "        faces = self.face_cascade.detectMultiScale(img, 1.2, 5)\n",
    "        eye_boxes: List[Tuple[int,int,int,int]] = []\n",
    "        for (x, y, fw, fh) in faces:\n",
    "            roi = img[y:y+fh, x:x+fw]\n",
    "            eyes = self.eye_cascade.detectMultiScale(roi)\n",
    "            for (ex, ey, ew, eh) in eyes[:2]:\n",
    "                eye_boxes.append((x+ex, y+ey, x+ex+ew, y+ey+eh))\n",
    "        return eye_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02312e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Dataset preparation ---------------------------- #\n",
    "\n",
    "class YOLODatasetBuilder:\n",
    "    def __init__(self, paths: Paths, img_size: int = 640):\n",
    "        self.p = paths\n",
    "        self.img_size = img_size\n",
    "        self.roi = ROIExtractor()\n",
    "        self.yolo_dirs = [\n",
    "            self.p.yolo_dataset/'images'/'train',\n",
    "            self.p.yolo_dataset/'images'/'val',\n",
    "            self.p.yolo_dataset/'labels'/'train',\n",
    "            self.p.yolo_dataset/'labels'/'val',\n",
    "        ]\n",
    "        for d in self.yolo_dirs:\n",
    "            ensure_dir(d)\n",
    "        ensure_dir(self.p.tmp)\n",
    "\n",
    "    def _collect_images(self) -> List[Tuple[Path, int]]:\n",
    "        pairs: List[Tuple[Path,int]] = []\n",
    "        for img_path in sorted(self.p.open_eyes.glob('**/*.*'), key=lambda s: natural_key(str(s))):\n",
    "            if img_path.suffix.lower() in {'.jpg', '.jpeg', '.png'}:\n",
    "                pairs.append((img_path, CLASS_MAP['open_eye']))\n",
    "        for img_path in sorted(self.p.closed_eyes.glob('**/*.*'), key=lambda s: natural_key(str(s))):\n",
    "            if img_path.suffix.lower() in {'.jpg', '.jpeg', '.png'}:\n",
    "                pairs.append((img_path, CLASS_MAP['closed_eye']))\n",
    "        return pairs\n",
    "\n",
    "    def _frames_from_videos(self) -> List[Tuple[Path, int]]:\n",
    "        results: List[Tuple[Path,int]] = []\n",
    "        for folder in [self.p.videos, self.p.new_videos]:\n",
    "            if not folder.exists():\n",
    "                continue\n",
    "            for v in sorted(folder.glob('**/*.*')):\n",
    "                if v.suffix.lower() not in {'.mp4', '.avi', '.mov', '.mkv'}:\n",
    "                    continue\n",
    "                name = v.stem.lower()\n",
    "                label = None\n",
    "                if any(k in name for k in ['open', 'mo']):\n",
    "                    label = CLASS_MAP['open_eye']\n",
    "                elif any(k in name for k in ['closed', 'dong', 'nhammat', 'sleep', 'drowsy']):\n",
    "                    label = CLASS_MAP['closed_eye']\n",
    "                if label is None:\n",
    "                    continue\n",
    "                cap = cv2.VideoCapture(str(v))\n",
    "                if not cap.isOpened():\n",
    "                    continue\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                step = max(frame_count // 40, 1)  # l·∫•y t·ªëi ƒëa 40 frame/video\n",
    "                idx = 0\n",
    "                saved = 0\n",
    "                while True:\n",
    "                    ok = cap.grab()\n",
    "                    if not ok:\n",
    "                        break\n",
    "                    if idx % step == 0:\n",
    "                        ok, frame = cap.retrieve()\n",
    "                        if not ok:\n",
    "                            break\n",
    "                        fp = self.p.tmp / f\"{v.stem}_{idx}.jpg\"\n",
    "                        cv2.imwrite(str(fp), frame)\n",
    "                        results.append((fp, label))\n",
    "                        saved += 1\n",
    "                        if saved >= 40:\n",
    "                            break\n",
    "                    idx += 1\n",
    "                cap.release()\n",
    "        return results\n",
    "\n",
    "    def _auto_label(self, img_path: Path, cls_id: int, out_img: Path, out_label: Path):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            return False\n",
    "        h, w = img.shape[:2]\n",
    "        # gi·ªØ nguy√™n ·∫£nh g·ªëc; YOLO s·∫Ω letterbox sau\n",
    "        cv2.imwrite(str(out_img), img)\n",
    "        boxes = self.roi.infer_eye_boxes(img)\n",
    "        ok_any = False\n",
    "        for box in boxes:\n",
    "            ok_any |= write_yolo_label(out_label, cls_id, box, w, h)\n",
    "        return ok_any\n",
    "\n",
    "    def build(self, val_ratio: float = 0.2):\n",
    "        print('üîÑ ƒêang gom ·∫£nh‚Ä¶')\n",
    "        pairs = self._collect_images()\n",
    "        pairs += self._frames_from_videos()\n",
    "        if len(pairs) == 0:\n",
    "            print('‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh/video!')\n",
    "            return\n",
    "        labels = [c for _, c in pairs]\n",
    "        train_pairs, val_pairs = train_test_split(pairs, test_size=val_ratio, random_state=SEED, stratify=labels)\n",
    "\n",
    "        for split, items in [('train', train_pairs), ('val', val_pairs)]:\n",
    "            img_dir = self.p.yolo_dataset/'images'/split\n",
    "            lbl_dir = self.p.yolo_dataset/'labels'/split\n",
    "            for src, cls_id in items:\n",
    "                out_img = img_dir / f\"{src.stem}.jpg\"\n",
    "                out_lbl = lbl_dir / f\"{src.stem}.txt\"\n",
    "                ensure_dir(out_img.parent)\n",
    "                ensure_dir(out_lbl.parent)\n",
    "                success = self._auto_label(src, cls_id, out_img, out_lbl)\n",
    "                if not success:\n",
    "                    # fallback: n·∫øu kh√¥ng ph√°t hi·ªán m·∫Øt, d√πng bbox m·∫∑t to√†n ·∫£nh (k√©m l√Ω t∆∞·ªüng nh∆∞ng c√≤n h∆°n b·ªè)\n",
    "                    img = cv2.imread(str(src))\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    h, w = img.shape[:2]\n",
    "                    cv2.imwrite(str(out_img), img)\n",
    "                    # KH√îNG khuy·∫øn ngh·ªã: ch·ªâ ƒë√≥ng vai tr√≤ d·ª± ph√≤ng\n",
    "                    write_yolo_label(out_lbl, cls_id, (0, 0, w-1, h-1), w, h)\n",
    "\n",
    "        # t·∫°o data.yaml\n",
    "        data_yaml = self.p.yolo_dataset/'data.yaml'\n",
    "        data_yaml.write_text(\n",
    "            \"\\n\".join([\n",
    "                f\"path: {self.p.yolo_dataset}\",\n",
    "                \"train: images/train\",\n",
    "                \"val: images/val\",\n",
    "                f\"nc: {len(CLASS_NAMES)}\",\n",
    "                f\"names: {CLASS_NAMES}\",\n",
    "            ]) + \"\\n\"\n",
    "        )\n",
    "        print(f\"‚úÖ Ho√†n t·∫•t build dataset ‚Üí {self.p.yolo_dataset}\")\n",
    "        # d·ªçn tmp\n",
    "        clean_tmp(self.p.tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2ea8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Trainer ---------------------------- #\n",
    "\n",
    "class YOLOTrainer:\n",
    "    def __init__(self, paths: Paths, model: str = 'yolov8n.pt', imgsz: int = 640):\n",
    "        self.p = paths\n",
    "        ensure_dir(paths.model_dir)\n",
    "        self.imgsz = imgsz\n",
    "        print(f\"üöÄ Loading model: {model}\")\n",
    "        self.model = YOLO(model)\n",
    "\n",
    "    def train(self,\n",
    "              epochs: int = 100,\n",
    "              batch: int | str = 'auto',\n",
    "              workers: int = 8,\n",
    "              lr0: float = 0.002,  # h∆°i th·∫•p cho ·ªïn ƒë·ªãnh\n",
    "              lrf: float = 0.12,   # cosine final lr ratio\n",
    "              mosaic: float = 0.3, # ƒë·ªëi t∆∞·ª£ng nh·ªè: gi·ªØ mosaic nh·∫π\n",
    "              mixup: float = 0.0,\n",
    "              hsv_h: float = 0.015,\n",
    "              hsv_s: float = 0.7,\n",
    "              hsv_v: float = 0.4,\n",
    "              degrees: float = 5.0,\n",
    "              translate: float = 0.05,\n",
    "              scale: float = 0.2,\n",
    "              shear: float = 1.0,\n",
    "              erasing: float = 0.0,\n",
    "              patience: int = 30,\n",
    "              device: str = None):\n",
    "        data_yaml = self.p.yolo_dataset/'data.yaml'\n",
    "        assert data_yaml.exists(), \"data.yaml ch∆∞a t·ªìn t·∫°i, h√£y ch·∫°y --prepare tr∆∞·ªõc\"\n",
    "\n",
    "        args = dict(\n",
    "            data=str(data_yaml),\n",
    "            imgsz=self.imgsz,\n",
    "            epochs=epochs,\n",
    "            batch=batch,\n",
    "            workers=workers,\n",
    "            device=device or (0 if cv2.cuda.getCudaEnabledDeviceCount() > 0 else 'cpu'),\n",
    "            seed=SEED,\n",
    "            project=str(self.p.output),\n",
    "            name='drowsy_det',\n",
    "            cos_lr=True,\n",
    "            lr0=lr0,\n",
    "            lrf=lrf,\n",
    "            optimizer='SGD',  # SGD + cosine th∆∞·ªùng b·ªÅn v·ªõi small objects; c√≥ th·ªÉ th·ª≠ AdamW\n",
    "            momentum=0.937,\n",
    "            weight_decay=0.0005,\n",
    "            warmup_epochs=3.0,\n",
    "            warmup_momentum=0.8,\n",
    "            warmup_bias_lr=0.1,\n",
    "            amp=True,\n",
    "            patience=patience,\n",
    "            cache='ram',  # tƒÉng t·ªëc IO\n",
    "            # Augment (theo ƒë·∫∑c th√π m·∫Øt nh·ªè)\n",
    "            mosaic=mosaic,\n",
    "            mixup=mixup,\n",
    "            hsv_h=hsv_h,\n",
    "            hsv_s=hsv_s,\n",
    "            hsv_v=hsv_v,\n",
    "            degrees=degrees,\n",
    "            translate=translate,\n",
    "            scale=scale,\n",
    "            shear=shear,\n",
    "            erasing=erasing,\n",
    "            box=7.5,  # tƒÉng nh·∫π loss bbox\n",
    "            cls=0.5,  # gi·∫£m tr·ªçng s·ªë cls ƒë·ªÉ tr√°nh overfit label noise\n",
    "            fl_gamma=1.5,  # focal loss\n",
    "            iou=0.2,  # iou loss gain\n",
    "            imgsz_min=self.imgsz,\n",
    "            imgsz_max=self.imgsz,\n",
    "            save_json=False,\n",
    "            val=True,\n",
    "        )\n",
    "        print('üìä Training args:', json.dumps({k:v for k,v in args.items() if k not in {'data'}}, indent=2))\n",
    "        results = self.model.train(**args)\n",
    "        save_dir = Path(results.save_dir)\n",
    "        best = save_dir/'weights'/'best.pt'\n",
    "        if best.exists():\n",
    "            dst = self.p.model_dir/'best_drowsy.pt'\n",
    "            shutil.copy2(best, dst)\n",
    "            print(f\"‚úÖ Best model ‚Üí {dst}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y best.pt!\")\n",
    "        return str(best)\n",
    "\n",
    "    def validate(self, weights: Path | None = None):\n",
    "        w = Path(weights) if weights else (self.p.model_dir/'best_drowsy.pt')\n",
    "        print(f\"üîé ƒê√°nh gi√°: {w}\")\n",
    "        m = YOLO(str(w))\n",
    "        data_yaml = self.p.yolo_dataset/'data.yaml'\n",
    "        return m.val(data=str(data_yaml), imgsz=self.imgsz, iou=0.6, conf=0.25)\n",
    "\n",
    "    def export(self, weights: Path | None = None, fmt: str = 'onnx'):\n",
    "        w = Path(weights) if weights else (self.p.model_dir/'best_drowsy.pt')\n",
    "        m = YOLO(str(w))\n",
    "        file = m.export(format=fmt)\n",
    "        dst = self.p.model_dir/f'drowsy_export.{fmt}'\n",
    "        shutil.copy2(file, dst)\n",
    "        print(f\"‚úÖ Exported ‚Üí {dst}\")\n",
    "        return str(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64df07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------- Realtime tester ---------------------------- #\n",
    "\n",
    "class Realtime:\n",
    "    def __init__(self, weights: Path, device: str | int | None = None):\n",
    "        self.model = YOLO(str(weights))\n",
    "        self.device = device or (0 if cv2.cuda.getCudaEnabledDeviceCount()>0 else 'cpu')\n",
    "\n",
    "    def run(self, src=0, conf=0.4):\n",
    "        cap = cv2.VideoCapture(src)\n",
    "        if not cap.isOpened():\n",
    "            print('‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c camera/video')\n",
    "            return\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            res = self.model.predict(frame, conf=conf, verbose=False, device=self.device)\n",
    "            for r in res:\n",
    "                for b in r.boxes:\n",
    "                    x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())\n",
    "                    cls = int(b.cls[0])\n",
    "                    cf = float(b.conf[0])\n",
    "                    label = f\"{CLASS_NAMES[cls]} {cf:.2f}\"\n",
    "                    cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "                    cv2.putText(frame, label, (x1, max(20, y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "            cv2.imshow('Drowsy Detect ‚Äì YOLO', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ae9694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model: yolov8n.pt\n"
     ]
    }
   ],
   "source": [
    "# --- thay th·∫ø ƒëo·∫°n parse_args + main c≈© ---\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    ap = argparse.ArgumentParser(description='Drowsy YOLO pipeline')\n",
    "    ap.add_argument('--prepare', action='store_true', help='Chu·∫©n ho√° & auto-label dataset YOLO')\n",
    "    ap.add_argument('--train', action='store_true', help='Hu·∫•n luy·ªán m√¥ h√¨nh')\n",
    "    ap.add_argument('--val', action='store_true', help='ƒê√°nh gi√° m√¥ h√¨nh')\n",
    "    ap.add_argument('--export', action='store_true', help='Xu·∫•t ONNX/TorchScript')\n",
    "    ap.add_argument('--realtime', action='store_true', help='Ch·∫°y realtime')\n",
    "    ap.add_argument('--weights', type=str, default='yolov8n.pt', help='Kh·ªüi t·∫°o/ƒë·ªçc weights')\n",
    "    ap.add_argument('--imgsz', type=int, default=640, help='K√≠ch th∆∞·ªõc ·∫£nh train/infer')\n",
    "    ap.add_argument('--fmt', type=str, default='onnx', choices=['onnx','torchscript','pt'])\n",
    "    ap.add_argument('--val_ratio', type=float, default=0.2)\n",
    "\n",
    "    # Khi ch·∫°y trong Jupyter/Spyder, b·ªè h·∫øt argv ‚Äúl·∫°‚Äù ƒë·ªÉ tr√°nh nu·ªët nh·∫ßm\n",
    "    if argv is None:\n",
    "        if any(m in sys.modules for m in ('ipykernel', 'spyder_kernels')):\n",
    "            argv = []\n",
    "        else:\n",
    "            argv = sys.argv[1:]\n",
    "\n",
    "    args, unknown = ap.parse_known_args(argv)\n",
    "    if unknown:\n",
    "        print(f\"‚ö†Ô∏è Ignoring unknown args from environment: {unknown}\")\n",
    "    return args\n",
    "\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    p = Paths.from_env()\n",
    "    for d in [p.open_eyes, p.closed_eyes, p.videos, p.new_videos, p.yolo_dataset, p.output, p.model_dir, p.tmp]:\n",
    "        ensure_dir(d)\n",
    "\n",
    "    if args.prepare:\n",
    "        YOLODatasetBuilder(p, img_size=args.imgsz).build(val_ratio=args.val_ratio)\n",
    "\n",
    "    trainer = YOLOTrainer(p, model=args.weights, imgsz=args.imgsz)\n",
    "\n",
    "    if args.train:\n",
    "        trainer.train()\n",
    "    if args.val:\n",
    "        trainer.validate()\n",
    "    if args.export:\n",
    "        trainer.export(fmt=args.fmt)\n",
    "    if args.realtime:\n",
    "        weights = p.model_dir/'best_drowsy.pt'\n",
    "        if not weights.exists():\n",
    "            print('‚ö†Ô∏è Ch∆∞a th·∫•y best_drowsy.pt, d√πng weights ƒë√£ cung c·∫•p')\n",
    "            weights = Path(args.weights)\n",
    "        Realtime(weights).run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
