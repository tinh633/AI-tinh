# law_search.py - 
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import urllib.parse
import logging
import time

# C·∫•u h√¨nh log
logging.basicConfig(level=logging.INFO)
log = logging.getLogger("LawSearch")

def search_vbpl_sync(user_query):
    """
    T√¨m ki·∫øm tr√™n Th∆∞ Vi·ªán Ph√°p Lu·∫≠t.
    Phi√™n b·∫£n V10: Th√™m c∆° ch·∫ø "V√©t c·∫°n" n·ªôi dung n·∫øu kh√¥ng t√¨m th·∫•y th·∫ª div ch√≠nh.
    """
    # 1. L√†m s·∫°ch c√¢u h·ªèi
    clean_query = user_query.replace("?", "").strip()
    
    # 2. T·ª™ KH√ìA M·ªöI NH·∫§T 2025
    search_query = f"site:thuvienphapluat.vn {clean_query} m·ª©c ph·∫°t m·ªõi nh·∫•t 2025"
    
    encoded_query = urllib.parse.quote(search_query)
    ddg_url = f"https://duckduckgo.com/?q={encoded_query}&t=h_&ia=web"
    
    print(f"üöÄ [TVPL Search] ƒêang t√¨m: '{search_query}'")

    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=False, slow_mo=1000)
        except Exception as e:
            return f"L·ªói: Ch∆∞a c√†i tr√¨nh duy·ªát. {e}"
            
        page = browser.new_page()
        
        try:
            # --- B∆Ø·ªöC 1: V√ÄO DUCKDUCKGO ---
            print(f"üåç Truy c·∫≠p Search Engine...")
            page.goto(ddg_url, timeout=30000, wait_until="domcontentloaded")
            time.sleep(3) 

            content = page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            found_link = None
            found_title = ""
            
            # --- B∆Ø·ªöC 2: L·ªåC LINK ---
            print("üîé ƒêang l·ªçc link k·∫øt qu·∫£...")
            all_links = soup.find_all("a", href=True)
            
            for a in all_links:
                href = a.get("href")
                title = a.get_text().strip()
                
                if not href.startswith("http"): continue

                if "thuvienphapluat.vn" in href:
                    if any(x in href for x in ["google", "search", "dang-nhap"]): continue
                    
                    print(f"   Link ·ª©ng vi√™n: {title[:50]}... -> {href}")
                    
                    found_link = href
                    found_title = title
                    break 

            if not found_link:
                return "Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt ph√π h·ª£p tr√™n Th∆∞ Vi·ªán Ph√°p Lu·∫≠t."

            # --- B∆Ø·ªöC 3: ƒê·ªåC B√ÄI VI·∫æT (C·∫¢I TI·∫æN) ---
            print(f"üéØ ƒê·ªçc b√†i: {found_link}")
            page.goto(found_link, timeout=30000, wait_until="domcontentloaded")
            
            # Ch·ªù n·ªôi dung (th·ª≠ nhi·ªÅu selector h∆°n)
            try:
                page.wait_for_selector("div.content-0, div.news-content, div.content, article", timeout=5000)
            except: pass

            sub_soup = BeautifulSoup(page.content(), "html.parser")
            
            # X√≥a c√°c ph·∫ßn r√°c tr∆∞·ªõc khi l·ªçc n·ªôi dung ƒë·ªÉ tr√°nh l·∫•y nh·∫ßm
            for tag in sub_soup(["script", "style", "div.relate-news", "div.comment", "div.adv", "div.bottom-mobile", "footer", "header"]):
                tag.decompose()

            # Th·ª≠ danh s√°ch selector m·ªü r·ªông (qu√©t h·∫øt c√°c ki·ªÉu layout c·ªßa TVPL)
            selectors = [
                "div.content-0", 
                "div.news-content", 
                "div.content", 
                "div#news-content",
                "article",
                "div.post-content"
            ]
            
            body = None
            for sel in selectors:
                body = sub_soup.select_one(sel)
                if body: break
            
            text = ""
            if body:
                text = " ".join(body.get_text(" ", strip=True).split())
            else:
                # FALLBACK: N·∫øu kh√¥ng kh·ªõp selector n√†o, l·∫•y to√†n b·ªô text trong body
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y div ch√≠nh, d√πng ch·∫ø ƒë·ªô ƒë·ªçc th√¥ (Raw Text)...")
                body_tag = sub_soup.find("body")
                if body_tag:
                    text = " ".join(body_tag.get_text(" ", strip=True).split())
                else:
                    return f"L·ªói: Trang web tr·ªëng r·ªóng ({found_link})"

            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ (C·∫Øt b·ªõt n·∫øu qu√° d√†i ƒë·ªÉ tr√°nh l·ªói API Chat)
            return f"Ngu·ªìn: Th∆∞ Vi·ªán Ph√°p Lu·∫≠t\nLink: {found_link}\n\nN·ªòI DUNG CHI TI·∫æT:\n{text[:12000]}..."

        except Exception as e:
            # browser.close() # ƒê√≥ng ·ªü finally r·ªìi
            return f"L·ªói qu√° tr√¨nh t√¨m ki·∫øm: {e}"
            
        finally:
             browser.close()

if __name__ == "__main__":
    print(search_vbpl_sync("v∆∞·ª£t ƒë√®n ƒë·ªè ph·∫°t bao nhi√™u"))